{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhXftgunS5rb"
      },
      "source": [
        "# Application of Optuna to find the optimal hyperparameters for transfer learning or fine tuning the pre-trained models\n",
        "\n",
        "@authors:\n",
        "\n",
        "Ali Aghababaei, aaghababaei78@gmail.com  and   Roya Arian, royaarian101@gmail.com\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8pgQHwRbnbn"
      },
      "source": [
        "This code was used to find best hyperparameters to classify MS and Normal cases using SLO images. However it can be used in any other application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzlrpgcVBvZW",
        "outputId": "e6e89220-73b2-4de6-d9ec-30b4d807ed6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QVxu8DsY9Rs"
      },
      "source": [
        "### Enter the file path in your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIwkl6vdMka-",
        "outputId": "1a3ef72d-5007-4bb4-9d17-0067480b130e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Slo_classification\n"
          ]
        }
      ],
      "source": [
        "%cd '/Enter your path'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tfe-hAeZtFU"
      },
      "source": [
        "### Installing Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65hNAiGd_iNU"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from optuna.integration import TFKerasPruningCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R68NhSlOKK1G"
      },
      "outputs": [],
      "source": [
        "####################### transfer learning ##############################\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "\n",
        "\n",
        "def classifier (trial):\n",
        "\n",
        "  # load one of the state-of-the-art models (VGG16, VGG19, Inception-V3, Resnet50, IfitientNet, efficientnet_b0 to b7)\n",
        "  base_model = tf.keras.applications.VGG19(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(128, 128, 3),\n",
        "        )\n",
        "\n",
        "\n",
        "  ################## fine tune\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "  # base_model.trainable = False\n",
        "\n",
        "  model = Sequential ()\n",
        "\n",
        "  model.add (base_model)\n",
        "\n",
        "  model.add (Flatten ())\n",
        "\n",
        "  dropout_l0 = trial.suggest_float(\"dropout_l0\", 0, 0.7,step=0.1)\n",
        "\n",
        "  model.add (Dropout (dropout_l0))\n",
        "\n",
        "\n",
        "  n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "\n",
        "\n",
        "  for i in range(n_layers):\n",
        "\n",
        "      n_units = trial.suggest_int(\"n_units_l{}\".format(i), 8, 5000, log = True)\n",
        "\n",
        "      model.add (Dense (n_units, activation = 'relu'))\n",
        "\n",
        "      dropout = trial.suggest_float(\"dropout_l{}\".format (i+1), 0, 0.7,step=0.1)\n",
        "\n",
        "      model.add (Dropout (rate = dropout))\n",
        "\n",
        "  model.add (Dense (1, activation = 'sigmoid'))\n",
        "\n",
        "  lr = trial.suggest_float ('lr', 1e-5, 1e-3, log = True)\n",
        "\n",
        "  my_optimizer = tf.keras.optimizers.Adam (learning_rate= lr)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=my_optimizer, loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ3Ot6a8apjU"
      },
      "source": [
        "### Applying subject-wise approach to split validation and train data\n",
        "\n",
        "To prevent data leakage between train and validation datasets, a “subject-wise” approach was followed that involves putting all images belonging to each\n",
        "individual, regardless of its left-or-right orientation, in a single group.\n",
        "\n",
        "\n",
        "### Creating Dataset\n",
        "\n",
        "Create a dictionary for your dataset in which each key refers to one patient and the value of each key must be a nested-dictionary with own key and value. Keyes in nested dictionary indicate the number of images belonging to the patient and values are the corresponding numpy array of the images. Save this dictionary as a pickle file named (“subjects_slo_data.pkl”). However, the label dictionary contains keys and values, where the keys are the same as the keys in the image dictionary and the values are the patient’s label. Save this dictionary as a pickle file named (“labels_slo_data.pkl”)\n",
        "For example patient number one has 4 images with label = 1 and patient number two has 2 images with label = 0. Therefore, the corresponding dictionaries are as follow:\n",
        "\n",
        "•\timages [0] is a dictionary with size (4):\n",
        "\n",
        "•\tnp.shape(images[0][0])  = (128 ×128 ×3)\n",
        "\n",
        "•\tnp.shape(images [0][1]) = (128 ×128 ×3)\n",
        "\n",
        "•\tnp.shape(images [0][2]) = (128 ×128 ×3)\n",
        "\n",
        "•\tnp.shape(images [0][3]) = (128 ×128 ×3)\n",
        "\n",
        "\n",
        "\tlabels_train [0] = 1\n",
        "\n",
        "\n",
        "\n",
        "•\timages [1] is a dictionary with size (2):\n",
        "\n",
        "•\tnp.shape(images [1][0]) = (128 ×128 ×3)\n",
        "\n",
        "•\tnp.shape(images [1][1]) = (128 ×128 ×3)\n",
        "\n",
        "\n",
        "\tlabels_train [1] = 0\n",
        "\n",
        "\n",
        "Remember to resize your images to (128 × 128 × 3) or change the input_shape in previous cell according to your desired size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQGBYPXmHuVx"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)\n",
        "\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "            data.append(x[i][j])\n",
        "            label.append(y[i])\n",
        "\n",
        "    data = np.reshape(data, np.shape(data))\n",
        "    return data, label\n",
        "\n",
        "\n",
        "def objective (trial):\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # load the data and the coresponding lables pickle file\n",
        "  images_train = pickle.load(open(\"subjects_slo_data.pkl\", 'rb'))\n",
        "  labels_train = pickle.load(open(\"labels_slo_data.pkl\", 'rb'))\n",
        "\n",
        "  train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "\n",
        "  x_train = {i: images_train[i] for i in train_index}\n",
        "\n",
        "  x_valid = {i: images_train[i] for i in val_index}\n",
        "\n",
        "  y_train = {i: labels_train[i] for i in train_index}\n",
        "\n",
        "  y_valid = {i: labels_train[i] for i in val_index}\n",
        "\n",
        "  x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "  x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "  # Normalizing\n",
        "  x_train /= 255\n",
        "  x_valid /= 255\n",
        "\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "  rotation_range= 5, # rotation\n",
        "  width_shift_range= [-30, 30], # horizontal shift\n",
        "  height_shift_range= [-5, 5] , # vertical shift\n",
        "  zoom_range= 0.2,\n",
        "  vertical_flip= True , # vertical flip\n",
        "  brightness_range= [0.2, 1.5]\n",
        "    )\n",
        "\n",
        "\n",
        "  batch=np.zeros_like(x_train, dtype=np.float32)\n",
        "\n",
        "  batch_label=np.zeros_like(y_train, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train)):\n",
        "\n",
        "    x1= x_train[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    batch[i,:,:,:] = x.next()\n",
        "\n",
        "    batch_label[i] = y_train[i]\n",
        "\n",
        "  x_train = np.concatenate([x_train,batch])\n",
        "\n",
        "  y_train = np.concatenate([y_train,batch_label])\n",
        "\n",
        "        ####################################################################\n",
        "        # classification via my model\n",
        "        ####################################################################\n",
        "\n",
        "\n",
        "  model = classifier (trial)\n",
        "\n",
        "\n",
        "  # Generate our trial model.\n",
        "\n",
        "  model.fit(x_train,\n",
        "            np.asarray(y_train, dtype=np.float64),\n",
        "            batch_size= batch_size,\n",
        "            epochs=50,\n",
        "            callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\"), EarlyStopping(patience=10, verbose=1),\n",
        "        ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "        ModelCheckpoint(f'slo.h5', verbose=1, save_best_only=True, save_weights_only=True)],\n",
        "            validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)),\n",
        "            )\n",
        "  model.load_weights(f'slo.h5')\n",
        "  score = model.evaluate (x_valid, np.asarray(y_valid, dtype=np.float64), verbose = 1)\n",
        "\n",
        "  return score [1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5FGtbZdzHXV"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
